{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple calls\n",
        "\n",
        "The hack that works here, is that `commit_files` takes a json string that looks like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "   \"files:\n",
        "   [\n",
        "      {\"path\" : \"path/to file.txt\", \"code\" : \"foo baz\" }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "It's super hacky, and depends on the model spitting out a very large an syntactically correct json.\n",
        "\n",
        "If you're here, you really should be thinking long and hard about workflows at this point.\n",
        "\n",
        "Additionally, this three step process is also pushing llama-3.3-70b to its limit, and about 20% of the time it stops before actually writing the code or committing (another reason to be using workflows).\n",
        "\n",
        "Note again the `system_prompt` which is the key to making these things work. You need to carefully explain the workflow step by step, and which tool to use at each step."
      ],
      "metadata": {
        "id": "Oe31EqsA10lp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujvVmMa0btJb",
        "outputId": "8e2a0e11-2fb8-49c2-cfc8-ef30b46aee1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m538.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama_toolbox (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall git+https://github.com/rawkintrevo/llama-toolbox.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import json\n",
        "# Create an OpenAI client with your deepinfra token and endpoint\n",
        "openai = OpenAI(\n",
        "    api_key=userdata.get('deepinfra'),\n",
        "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        ")\n",
        "\n",
        "from llama_toolbox.github import GetRepoContents, \\\n",
        "  ReadIssue, \\\n",
        "  CommitFiles\n",
        "from llama_toolbox.basic_math import complex_response\n",
        "\n",
        "\n",
        "tool_list = [F(api_key = userdata.get('github-token')) for F in [GetRepoContents,\n",
        "                                                                 ReadIssue, ]]\n",
        "cf = CommitFiles(api_key=userdata.get('github-token'), git_user_email='trevor.d.grant@gmail.com', git_user_name='bottrevo')\n",
        "tool_list.append(cf)\n",
        "\n",
        "tool_map = {f.name: f.fn for f in tool_list}\n",
        "tool_desc_list = [f.definition for f in tool_list]"
      ],
      "metadata": {
        "id": "JBUKOfaFcSlF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential Actions need to be defined in the\n",
        "system_prompt = \"\"\"\n",
        "To analyze code then create issues is a multi step process:\n",
        "Step 1 : Get the code (use get_repo_contents). Only do this once, for the entire repo.\n",
        "Step 2 : Get the issue and comments (use read_issue)\n",
        "Step 3 : Fix the issue, and be very careful to pass the full and correct path in a json string to commit_files (use commit_files)\n",
        "Do not terminate until all three steps have been completed.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"In the repository 'https://github.com/rawkintrevo/llama-toolbox/', make two new text files with names 'foo.txt' and 'baz.txt', fill each up with some babble, and commit them to a branch called 'commit_files_demo'\"\n",
        "\n",
        "\n",
        "out = complex_response(prompt=user_prompt,\n",
        "                       system=system_prompt,\n",
        "                       tool_desc_list=tool_desc_list,\n",
        "                       openai_like_client=openai,\n",
        "                       model_name= \"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "                       tool_map=tool_map,\n",
        "                       max_thoughts=5,\n",
        "                       temperature=0.1)\n",
        "\n",
        "print(out['last_response'].choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZpgZbRjUgb_",
        "outputId": "ef0dcb0b-a6b4-4bb3-fb2b-7a74b7c9c64c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thinking...\n",
            "This means the two files 'foo.txt' and 'baz.txt' have been successfully created and committed to the 'commit_files_demo' branch in the 'https://github.com/rawkintrevo/llama-toolbox' repository. The files contain the specified text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And so they have, see for yourself at https://github.com/rawkintrevo/llama-toolbox/tree/commit_files_demo"
      ],
      "metadata": {
        "id": "pPVqhug08_e9"
      }
    }
  ]
}